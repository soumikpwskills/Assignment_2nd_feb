{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.**"
      ],
      "metadata": {
        "id": "_RAEoLvUJupM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans :-**\n",
        "\n",
        "#### Web Scraping :-\n",
        "Web scraping refers to the extraction of data from a website. This information is collected and then exported into a format that is more useful for the user. Be it a spreadsheet or an API.\n",
        "\n",
        "Although web scraping can be done manually, in most cases, automated tools are preferred when scraping web data as they can be less costly and work at a faster rate.But in most cases, web scraping is not a simple task. Websites come in many shapes and forms, as a result, web scrapers vary in functionality and features.\n",
        "\n",
        "⚫ Web scraping has countless applications, especially within the field of data analytics. Market research companies use scrapers to pull data from social media or online forums for things like customer sentiment analysis. Others scrape data from product sites like Amazon or eBay to support competitor analysis.\n",
        "\n",
        "Meanwhile, Google regularly uses web scraping to analyze, rank, and index their content. Web scraping also allows them to extract information from third-party websites before redirecting it to their own (for instance, they scrape e-commerce sites to populate Google Shopping).\n",
        "\n",
        "Many companies also carry out contact scraping, which is when they scrape the web for contact information to be used for marketing purposes. If you’ve ever granted a company access to your contacts in exchange for using their services, then you’ve given them permission to do just this.\n",
        "\n",
        "⚫ Three areas where web scraping is used --\n",
        "1. **Price Monitoring :-**\n",
        "Web Scraping can be used by companies to scrap the product data for their products and competing products as well to see how it impacts their pricing strategies. Companies can use this data to fix the optimal pricing for their products so that they can obtain maximum revenue.\n",
        "\n",
        "2. **Market Research :-**\n",
        "Web scraping can be used for market research by companies. High-quality web scraped data obtained in large volumes can be very helpful for companies in analyzing consumer trends and understanding which direction the company should move in the future.\n",
        "\n",
        "3. **News Monitoring :-**\n",
        "Web scraping news sites can provide detailed reports on the current news to a company. This is even more essential for companies that are frequently in the news or that depend on daily news for their day-to-day functioning. After all, news reports can make or break a company in a single day!\n",
        "\n",
        "4. **Sentiment Analysis :-**\n",
        "If companies want to understand the general sentiment for their products among their consumers, then Sentiment Analysis is a must. Companies can use web scraping to collect data from social media websites such as Facebook and Twitter as to what the general sentiment about their products is. This will help them in creating products that people desire and moving ahead of their competition.\n",
        "\n",
        "5. **Email Marketing :-**\n",
        "Companies can also use Web scraping for email marketing. They can collect Email ID’s from various sites using web scraping and then send bulk promotional and marketing Emails to all the people owning these Email ID’s."
      ],
      "metadata": {
        "id": "q532-RDxJurO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What are the different methods used for Web Scraping?**"
      ],
      "metadata": {
        "id": "ETuQTdx6LxgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans :-**\n",
        "Different methods used for Web Scrapping are --\n",
        "\n",
        "**1. Human Copy-and-Paste :-**\n",
        "\n",
        "Manually copying and pasting data from a web page into a text file or spreadsheet is the most basic form of web scraping. Even the best web-scraping technology cannot always replace a human’s manual examination and copy-and-paste, and this may be the only viable option when the websites for scraping explicitly prohibit machine automation.\n",
        "\n",
        "**2. Text Pattern Matching :-**\n",
        "\n",
        "The UNIX grep command or regular expression-matching facilities of programming languages can be used to extract information from web pages in a simple yet powerful way (for instance Perl or Python).\n",
        "\n",
        "**3. HTTP Programming :-**\n",
        "\n",
        "Static and dynamic web pages can be retrieved by using socket programming to send HTTP requests to a remote web server.\n",
        "\n",
        "**4. HTML Parsing :-**\n",
        "\n",
        "Many websites contain large collections of pages that are dynamically generated from an underlying structured source, such as a database. A common script or template is typically used to encode data from the same category into similar pages. A wrapper is a program in data mining that detects such templates in a specific information source, extracts its content, and converts it to a relational form.\n",
        "\n",
        "Wrapper generation algorithms assume that the input pages of a wrapper induction system follow a common template and can be identified using a URL common scheme. Furthermore, semi-structured data query languages such as XQuery and HTQL can be used to parse HTML pages as well as retrieve and transform page content.\n",
        "\n",
        "**5. DOM Parsing :-**\n",
        "\n",
        "More information: Object Model for Documents, Programs can retrieve dynamic content generated by client-side scripts by embedding a full-fledged web browser, such as Internet Explorer or the Mozilla browser control. These browser controls also parse web pages into a DOM tree, which programs can use to retrieve portions of the pages. The resulting DOM tree can be parsed using languages such as Xpath.\n",
        "\n",
        "**6. Vertical Aggregation :-**\n",
        "\n",
        "Several companies have created vertically specific harvesting platforms. These platforms generate and monitor a plethora of “bots” for specific verticals with no “man in the loop” (direct human involvement) and no work related to a specific target site. The preparation entails creating a knowledge base for the entire vertical, after which the platform will create the bots automatically.\n",
        "\n",
        "The robustness of the platform is measured by the quality of the information it retrieves (typically the number of fields) and its scalability (how quickly it can scale up to hundreds or thousands of sites). This scalability is primarily used to target the Long Tail of sites that common aggregators find too difficult or time-consuming to harvest content from.\n",
        "\n",
        "**7. Semantic Annotation Recognizingn :-**\n",
        "\n",
        "The scraped pages may include metadata, semantic markups, and annotations that can be used to locate specific data snippets. This technique can be viewed as a subset of DOM parsing if the annotations are embedded in the pages, as Microformat does. In another case, the annotations are stored and managed separately from the web pages, so scrapers can retrieve data schema and instructions from this layer before scraping the pages.\n",
        "\n",
        "**8. Computer Vision Web-Page Analysis :-**\n",
        "\n",
        "There are efforts using machine learning and computer vision to identify and extract information from web pages by visually interpreting pages as a human would."
      ],
      "metadata": {
        "id": "RqZ2-HBAMI_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. What is Beautiful Soup? Why is it used?**"
      ],
      "metadata": {
        "id": "CoAa9JK1Nh1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans :- Beautiful Soup is a Python library that makes it easy to scrape information from web pages. It sits atop an HTML or XML parser and provides Pythonic idioms for iterating, searching, and modifying the parse tree.\n",
        "\n",
        "⚫ The Beautiful Soup library helps with isolating titles and links from webpages. It can extract all of the text from ​HTML tags, and alter the HTML ​in the document with which we’re working."
      ],
      "metadata": {
        "id": "Ufm6gB5DNoBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. Why is flask used in this Web Scraping project?**"
      ],
      "metadata": {
        "id": "h-CA2WnvT4bi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans :-** Flask is used in Web Scraping project because --\n",
        "\n",
        "* **Lightweight:** Flask is a lightweight framework because it is independent of external libraries and it gives a quick start for web development having complex applications.\n",
        "* **Compatible:** Flask is compatible with the latest technologies such as machine learning, agile development, cloud technologies, etc.\n",
        "* **Independent:** Flask allows full control to the developers for creating web applications. A developer can do the experiment with the libraries and architecture of the framework.\n",
        "* **Integrated Unit Testing:** Flask offers an integrated unit testing feature that helps in faster debugging, robust development, and independence to do experiments.\n",
        "* **Flexible and Scalable:** Flask supports WSGI templates that help in flexibility and scalability in the web development process.\n",
        "* **Secure Cookies:** Secure cookie is an attribute of an HTTP request that enables the security of channels and ensures no unauthorized person has access to the text. Flask supports the feature of secure cookies."
      ],
      "metadata": {
        "id": "__oyTpEAT96I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. Write the names of AWS services used in this project. Also, explain the use of each service.**"
      ],
      "metadata": {
        "id": "mnkWqtFvVff2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. AWS Beanstalk :-**\n",
        "Amazon Elastic Beanstalk is an AWS service used for deployment and scaling web applications developed using Java, PHP, Python, Docker, etc. It supports running and managing web applications. You just need to upload your code and the deployment part is handled by Elastic Beanstalk (from capacity provisioning, load balancing, and auto-scaling to application health monitoring). It is the best service for developers since it takes care of the servers, load balancers, and firewalls. Also, you can have control over AWS assets and the other resources required for the application. You get the benefit of paying for what you use, thus maintaining cost-effectiveness.  \n",
        "\n",
        "**2. AWS CodePipeline :-**\n",
        "AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipeline. It allows users to build, test and deploy code into a test or production environment using either the AWS CLI or a clean UI configuration process within the Amazon Console.Amazon Web Services CodePipeline is highly configurable and has a very short learning curve. Those familiar with the Amazon ecosystem will find it extremely easy to create a CICD pipeline for their applications and services.AWS CodePipeline leverages many of the management tools already in the AWS environment, such as AWS CodeCommit, AWS CodeStar, Amazon ECR, AWS Identity, Amazon ECS, AWS CDK, Amazon EC2 instance, AWS Management Console, Amazon Linux, AWS Step Functions and AWS CodeDeploy. It does not limit itself to aggregating only internal services. Users can also create integrations with tools and services like GitHub and Jenkins."
      ],
      "metadata": {
        "id": "_7DIBn2WWXUJ"
      }
    }
  ]
}